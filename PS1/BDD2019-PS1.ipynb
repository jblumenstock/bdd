{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDD Assignment #1\n",
    "\n",
    "The goal of this problem set is to replicate and extend the core results of  Jean et al.'s 2016 paper, \"Combining satellite imagery and machine learning to predict poverty.\" This problem set will be challenging and time-consuming, so I suggest you start immediately. Your first step should be to carefully read <a href=\"https://pdfs.semanticscholar.org/1b3a/c4b4187a3dbc9373869e7774b1dc63f748d2.pdf\">the original paper</a>  as well as the <a href=\"http://science.sciencemag.org/content/sci/suppl/2016/08/19/353.6301.790.DC1/Jean.SM.pdf\">supplementary materials</a>.\n",
    "\n",
    "For this assignment, we will focus on the country of Rwanda. You will need to download three distinct datasets, including DHS data, satellite data from the Google Maps API, as well as nighttime luminosity data. The DHS data requires registration (which can take several days to be approved), and the Google Maps API is rate-limited, so it will necessarily take you several days to download the requisite data, so make sure to **get started on those steps asap**. The deep learning section may also take several hours to compute (or days, if you have a slow computer), so don't save it until the last minute.\n",
    "\n",
    "## Overview of the problem set\n",
    "\n",
    "These are the key steps in the problem set:\n",
    "\n",
    "1. [Download satellite night lights images from NOAA](#step_1)\n",
    "2. [Download Rwandan DHS and construct cluster-level aggregates](#step_2)\n",
    "3. [Test whether night lights data can predict wealth, as observed in DHS](#step_3)\n",
    "4. [Download daytime satellite imagery from Google Maps](#step_4)\n",
    "5. [Test whether basic features of daytime imagery can predict wealth](#step_5)\n",
    "6. [*Optional*: Extract features from daytime imagery using deep learning libraries](#step_6)\n",
    "7. [*Optional*: Replicate final model and results of Jean et al (2016)](#step_7)\n",
    "8. [Construct a high-resolution map of the predicted wealth of Rwanda](#step_8)\n",
    "9. [Step back and interpret your results](#step_9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download satellite night lights images from NOAA\n",
    "\n",
    "- **INPUT**:\n",
    " - None\n",
    "- **OUTPUT**: \n",
    " - `F182010.v4d_web.stable_lights.avg_vis.tif`: Single image file giving nightlights intensity around the world\n",
    "\n",
    "Go to the [DMSP-OLS website](https://ngdc.noaa.gov/eog/dmsp/downloadV4composites.html) and download the satellite nighttime luminosity data (roughly 400MB). We will use the one from 2010. The archive they provide constains several files. Feel free to explore these files. We will only be using the file F182010.v4d_web.stable_lights.avg_vis.tif.\n",
    "\n",
    "A code snippet to get you started is below. Plot a map of nightlight luminosity in Rwanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "night_image_url = 'https://ngdc.noaa.gov/eog/data/web_data/v4composites/F182010.v4.tar'\n",
    "wget.download(night_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download Rwandan DHS and construct cluster-level aggregates\n",
    "\n",
    "- **INPUT**: \n",
    "  - `rwanda_clusters_location.csv`: Coordinates of the centroid of each cluster\n",
    "- **OUTPUT**: \n",
    "  - `rwanda_cluster_avg_asset_2010.csv`: Comma-delimited file indicated average wealth of each cluster \n",
    "\n",
    "[Demographic and Health Surveys (DHS)](http://dhsprogram.com/What-We-Do/Survey-Types/DHS.cfm) are nationally-representative household surveys that provide data for a wide range of monitoring and impact evaluation indicators in the areas of population, health, and nutrition. For this assignment, you will need to download the [2010 Rwandan DHS data](http://dhsprogram.com/what-we-do/survey/survey-display-364.cfm). **This requires registration, so start early!** Do not forget to request for the GPS dataset. Make sure you understand the structure of the data before starting.\n",
    "\n",
    "Your immediate goal is to take the raw survey data, covering 12,540 households, and compute the average household wealth for each survey cluster (think of a cluster as a village). Refer to the file `Recode6_DHS_22March2013_DHSG4.pdf` for information on these data.\n",
    "\n",
    "Save your output as `rwanda_cluster_avg_asset_2010.csv` and check that it matches the file that we have provided. You will use this file as input to the next step in the assignment.\n",
    "\n",
    "Conduct some basic statistical analysis of these data, for instance to show the wealth distribution of rwanda (e.g., as a histogram). If you have time, create a map showing the spatial distribution of wealth of Rwanda, as calculated from the DHS. Intpret this analysis -- what have you learned about the distribution of wealth in Rwanda?\n",
    "\n",
    "Hints:\n",
    "- `Household Recode` contains all the attributes of each household. It provides datasets with different formats. Feel free to explore the data. You can use `RWHR61FL.DAT` file in Flat ASCII data (.dat) format.\n",
    "- `RWHR61FL.DCF` describes the attributes and the location of each attribute.\n",
    "- Geographic Datasets: `rwge61fl.zip` contains the location of each cluster in Rwanda. It is in the format of shapefile, which needs QGIS or other GIS softwares to open. For those who are not familiar with GIS tools or who want a shortcut, you can also sue the file `rwanda_clusters_location.csv` provided with the problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the cluster locations, overlaid on the nightlights data, are shown in the figure below.\n",
    "<img src=\"figure/map1.png\" alt=\"Map\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Replace this section with your observations and interpretation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test whether night lights data can predict wealth, as observed in DHS\n",
    "\n",
    "Now that you have \"ground truth\" measures of average cluster wealth, your goal is to understand whether the nightlights data can be used to predict wealth. First, merge the DHS and nightlights data, and then fit a model of wealth on nightlights.\n",
    "\n",
    "## 3.1 Merge nightlights and DHS data at cluster level\n",
    "- **INPUT**: \n",
    " - `F182010.v4d_web.stable_lights.avg_vis.tif`: Nightlights data, from Step 1\n",
    " - `rwanda_cluster_avg_asset_2010.csv`: DHS cluster averages, from Step 2\n",
    "- **OUTPUT**: Merged datasets\n",
    " - `DHS_nightlights.csv`: Merged dataset with 492 rows, and 6 columns (one indicates average cluster wealth, 5 nightlights features)\n",
    " - Scatterplot of nightlights vs. DHS wealth\n",
    "\n",
    "Perform a \"spatial join\" to compute the average nighttime luminosity for each of the DHS clusters. To do this, you should take the average of the luminosity values for the nightlights locations surrounding the cluster centroid.\n",
    "\n",
    "Save your output as `DHS_nightlights.csv` and check that it is the same as the file we have provided.\n",
    "\n",
    "Create a scatterplot showing the relationship between average cluster wealth (y-axis) and average nighttime luminosity (x-axis). Your scatterplot should have one dot for each of the 492 DHS clusters. Report the R^2 of the regression line.\n",
    "\n",
    "Hints:\n",
    " - The resolution of each pixel in the nightlight image is about 1km. Use 10 pixels X 10 pixels to average the luminosity of each cluster.\n",
    " - Start by just taking the **Mean** of the luminosity in the 100 pixels and comparing this to cluster average wealth. If you like, you could also compute other luminosity characteristics of each cluster, such as the **Max**, **Min**, **Standard Deviation** of the 100 pixel values, but this step is not required. Note that the file we provide (`DHS_nightlights.csv`) has these added features.\n",
    " - To read the raw raster (nightlights) files, we recommend using the GDAL library. Use `conda install gdal` to install the GDAL library. We have provided some helper code for this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "import cStringIO\n",
    "gdal.UseExceptions()\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "import urllib\n",
    "\n",
    "# Helper function to read a raster file\n",
    "def read_raster(raster_file):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    read_raster\n",
    "\n",
    "    Given a raster file, get the pixel size, pixel location, and pixel value\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raster_file : string\n",
    "        Path to the raster file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_size : float\n",
    "        Pixel size\n",
    "    top_left_x_coords : numpy.ndarray  shape: (number of columns,)\n",
    "        Longitude of the top-left point in each pixel\n",
    "    top_left_y_coords : numpy.ndarray  shape: (number of rows,)\n",
    "        Latitude of the top-left point in each pixel\n",
    "    centroid_x_coords : numpy.ndarray  shape: (number of columns,)\n",
    "        Longitude of the centroid in each pixel\n",
    "    centroid_y_coords : numpy.ndarray  shape: (number of rows,)\n",
    "        Latitude of the centroid in each pixel\n",
    "    bands_data : numpy.ndarray  shape: (number of rows, number of columns, 1)\n",
    "        Pixel value\n",
    "    \"\"\"\n",
    "    raster_dataset = gdal.Open(raster_file, gdal.GA_ReadOnly)\n",
    "    # get project coordination\n",
    "    proj = raster_dataset.GetProjectionRef()\n",
    "    bands_data = []\n",
    "    # Loop through all raster bands\n",
    "    for b in range(1, raster_dataset.RasterCount + 1):\n",
    "        band = raster_dataset.GetRasterBand(b)\n",
    "        bands_data.append(band.ReadAsArray())\n",
    "        no_data_value = band.GetNoDataValue()\n",
    "    bands_data = np.dstack(bands_data)\n",
    "    rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "    # Get the metadata of the raster\n",
    "    geo_transform = raster_dataset.GetGeoTransform()\n",
    "    (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = geo_transform\n",
    "    \n",
    "    # Get location of each pixel\n",
    "    x_size = 1.0 / int(round(1 / float(x_size)))\n",
    "    y_size = - x_size\n",
    "    y_index = np.arange(bands_data.shape[0])\n",
    "    x_index = np.arange(bands_data.shape[1])\n",
    "    top_left_x_coords = upper_left_x + x_index * x_size\n",
    "    top_left_y_coords = upper_left_y + y_index * y_size\n",
    "    # Add half of the cell size to get the centroid of the cell\n",
    "    centroid_x_coords = top_left_x_coords + (x_size / 2)\n",
    "    centroid_y_coords = top_left_y_coords + (y_size / 2)\n",
    "\n",
    "    return (x_size, top_left_x_coords, top_left_y_coords, centroid_x_coords, centroid_y_coords, bands_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to get the pixel index of the point\n",
    "def get_cell_idx(lon, lat, top_left_x_coords, top_left_y_coords):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    get_cell_idx\n",
    "\n",
    "    Given a point location and all the pixel locations of the raster file,\n",
    "    get the column and row index of the point in the raster\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lon : float\n",
    "        Longitude of the point\n",
    "    lat : float\n",
    "        Latitude of the point\n",
    "    top_left_x_coords : numpy.ndarray  shape: (number of columns,)\n",
    "        Longitude of the top-left point in each pixel\n",
    "    top_left_y_coords : numpy.ndarray  shape: (number of rows,)\n",
    "        Latitude of the top-left point in each pixel\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lon_idx : int\n",
    "        Column index\n",
    "    lat_idx : int\n",
    "        Row index\n",
    "    \"\"\"\n",
    "    lon_idx = np.where(top_left_x_coords < lon)[0][-1]\n",
    "    lat_idx = np.where(top_left_y_coords > lat)[0][-1]\n",
    "    return lon_idx, lat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this illustrates how you can read the nightlight image\n",
    "raster_file = '/Users/guanghua/Downloads/F182010.v4/F182010.v4d_web.stable_lights.avg_vis.tif'\n",
    "x_size, top_left_x_coords, top_left_y_coords, centroid_x_coords, centroid_y_coords, bands_data = read_raster(raster_file)\n",
    "\n",
    "# save the result in compressed format - see https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html\n",
    "np.savez('nightlight.npz', top_left_x_coords=top_left_x_coords, top_left_y_coords=top_left_y_coords, bands_data=bands_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Fit a model of wealth as a function of nightlights\n",
    "- **INPUT**: \n",
    " - `DHS_nightlights.csv`, from Step 3.1\n",
    "- **OUTPUT**: \n",
    " - R^2 of model\n",
    " \n",
    "Above, you fit a regression line to illustrate the relationship between cluster average wealth and corresponding cluster nightlights. Now, use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) to get a better sense of out of sample accuracy. Report the cross-validated accuracy (R^2); how does it compare to the accuracy you reported in 3.1? How do you interpret these differences?\n",
    "\n",
    "There are two options for this. The basic way, for those new to machine learning, is to randomly divide your dataset into a training and a test dataset. Randomly select 80% of your clusters and fit a model of cluster-average DHS wealth (your response/dependent variable) on nightlights (your predictor/independent variables). You can use a regression or any other model you prefer. Then, use that model to predict the wealth of the remaining 20% of your data, and compare the predicted values to the actual values, and report the R^2 on these 20%.\n",
    "\n",
    "The preferred way is to use 10-fold cross-validation, where you repeat the above procedure 10 times, so that you have 10 different and non-overlapping test sets. Then, you report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds).\n",
    "\n",
    "Hints:\n",
    " - The scikit learn library has built-in functions for [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) that make this quite easy.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Replace this section with your observations and interpretation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Download daytime satellite imagery \n",
    "- **INPUT**: \n",
    " - Google Maps API key\n",
    " - `Sector_Boundary_2012.shp`: Rwandan shapefile\n",
    "- **OUTPUT**: \n",
    " - Thousands of satellite images (store in directory `google_image/`)\n",
    "\n",
    "We will use the Google Static Maps API to download satellite images. Refer [Google Static Maps introduction](https://developers.google.com/maps/documentation/static-maps/intro) and [Google Static Maps API Usage Limits](https://developers.google.com/maps/documentation/static-maps/usage-limits). You must apply for an API key before downloading. ** Note that it may take you several days to download the required images, so start early!**\n",
    "\n",
    "Download the images from Google at zoom level 16 (pixel resolution is about 2.5m). Set the image size to be 400 pixels X 400 pixels, so that each image you download will cover 1 square kilometer. In this way, each daytime image you download will correspond to a single pixel from the nighttime imagery from Step 1 above.\n",
    "\n",
    "Note: Google Map API requires that \"You must enable billing on each of your projects.\" Luckily, \" When you enable billing, you get \\$200 free usage every month for Maps, Routes, or Places. Based on the millions of users using our APIs today, most of them can continue to use Google Maps Platform for free with this credit.\" ([ref](https://cloud.google.com/maps-platform/user-guide/pricing-changes/)) You need to download about 50,000 images in this problem set, which costs about \\$100. So the \\$200 free usage every month should be enough. Remember to set a budget alert to track how your spend is growing toward a particular amount ([ref](https://cloud.google.com/billing/docs/how-to/budgets)).\n",
    "\n",
    "Hints:\n",
    " - You will need to tell Google the locations for which you wish to download images. One way to do this is to use a [shapefiles](https://en.wikipedia.org/wiki/Shapefile) that specifies the borders of Rwanda. We have provided this shapefile (`Sector_Boundary_2012.shp`) as well as a helper function to read in the shapefile.\n",
    " - The function we provide below does not limit the maximum number of images downloaded per day. Note that if you attempt to  download more than the daily limit, Google will return blank images instead of an error.\n",
    " - You can organize the files however you like. However, for later analysis (Steps 6 and beyond), it may help if you organize these daytime images into 64 folders, with one folder indicating the nightlight intensity of the pixel corresponding to the daytime image. In other words, if you download a daytime image for which the corresponding nighttime pixel has value 32, store that daytime image in a folder labeled '32'. This way, all the satellite images within each folder will have the same nightlight intensity. The file name is columnIndex_rowIndex.jpg, in which row index and column index are the index in the nightlight image (See the diagram below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figure/data_description.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to read a shapefile\n",
    "def get_shp_extent(shp_file):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    get_shp_extent\n",
    "\n",
    "    Given a shapefile, get the extent (boundaries)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp_file : string\n",
    "        Path to the shapefile\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    extent : tuple\n",
    "        Boundary location of the shapefile (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    import fiona\n",
    "    shape = fiona.open(shp_file)\n",
    "    extent = shape.bounds\n",
    "    return extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to download images from Google Maps API\n",
    "\n",
    "from retrying import retry\n",
    "\n",
    "# Stop after trying 10 time\n",
    "@retry(stop_max_attempt_number=10)\n",
    "def save_img(url, file_path, file_name):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    save_img\n",
    "\n",
    "    Given a url of the map, save the image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        URL of the map from Google Map Static API\n",
    "    file_path : string\n",
    "        Folder name of the map\n",
    "    file_name : string\n",
    "        File name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    a = urllib.urlopen(url).read()\n",
    "    b = cStringIO.StringIO(a)\n",
    "    image = ndimage.imread(b, mode='RGB')\n",
    "    # when no image exists, api will return an image with the same color. \n",
    "    # and in the center of the image, it said'Sorry. We have no imagery here'.\n",
    "    # we should drop these images if large area of the image has the same color.\n",
    "    if np.array_equal(image[:,:10,:],image[:,10:20,:]):\n",
    "        pass\n",
    "    else:\n",
    "        misc.imsave(file_path + file_name, image[50:450, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now read in the shapefile for Rwanda and extract the edges of the country\n",
    "inShapefile = \"provided/Sector_Boundary_2012/Sector_Boundary_2012.shp\"\n",
    "x_min_shp, y_min_shp, x_max_shp, y_max_shp = get_shp_extent(inShapefile)\n",
    "\n",
    "left_idx, top_idx = get_cell_idx(x_min_shp, y_max_shp, top_left_x_coords, top_left_y_coords)\n",
    "right_idx, bottom_idx = get_cell_idx(x_max_shp, y_min_shp, top_left_x_coords, top_left_y_coords)\n",
    "\n",
    "key = 'YOUR_GOOGLE_MAP_API_KEY'\n",
    "m = 1\n",
    "for i in xrange(left_idx, right_idx + 1):\n",
    "    for j in xrange(top_idx, bottom_idx + 1):\n",
    "        lon = centroid_x_coords[i]\n",
    "        lat = centroid_y_coords[j]\n",
    "        url = 'https://maps.googleapis.com/maps/api/staticmap?center=' + str(lat) + ',' + \\\n",
    "               str(lon) + '&zoom=16&size=400x500&maptype=satellite&key=' + key\n",
    "#         print(url)\n",
    "        lightness = bands_data[j, i, 0]\n",
    "        file_path = 'google_image/' + str(lightness) + '/'\n",
    "        if not os.path.isdir(file_path):\n",
    "            os.makedirs(file_path)\n",
    "        file_name = str(i) + '_' + str(j) +'.jpg'\n",
    "        save_img(url, file_path, file_name)\n",
    "        if m % 100 == 0:\n",
    "            print m\n",
    "        m += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test whether basic features of daytime imagery can predict wealth\n",
    "In step 3, you tested whether nightlight imagery could predict the wealth of Rwandan villages. You will now test whether daytime imagery can predict village wealth. Start by extracting simple metrics from the daytime imagery; in step 6 you will use more sophsticated methods to engineer these features from the images. **You don't need to do this step if you are able to do step 6.**\n",
    "\n",
    "## 5.1. Extract \"basic\" features from daytime imagery\n",
    "- **INPUT**: \n",
    " - `google_image/...`: Raw images, from Step 4\n",
    "- **OUTPUT**: \n",
    " - `google_image_features_basic.csv`: Image features \n",
    "\n",
    "Convert the raw data from the satellite imagery into a set of features that can be used in a machine learning algorithm. A simple way to do this is to take the raw R/G/B values for each pixel and average them for the image. Thus, if an image has 100 pixels, you will have an average R value, an average G value, and an average B value. Create more features by also computing the min, max, median, and standard deviation of R, G, and B for each image. This process will convert each image into a vector of 15 features.\n",
    "\n",
    "Feel free to be creative if you wish to generate additional features from the imagery -- this is similar to the process described in section 2.3 of the paper's supplementary materials. But don't waste too much time, and don't expect these features to be terribly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Merge daytime images with DHS data\n",
    "\n",
    "- **INPUT**: \n",
    " - `google_image_features_basic.csv`: Satellite imagery features, from Step 5.1\n",
    " - `rwanda_cluster_avg_asset_2010.csv`: DHS cluster averages, from Step 2\n",
    "- **OUTPUT**: Merged datasets\n",
    " - `data/model/DHS_daytime.csv`: Merged dataset with 492 rows, and 16 columns (one indicates average cluster wealth, 15 daytime image features)\n",
    "\n",
    "Now that you have feature vectors for each image, you should merge these with the DHS data indicated average cluster wealth. Follow a similar procedure as you did with 3.1, i.e., determine which image feature vectors are associated with each cluster, and then calculate, for each cluster, the average value of each feature.\n",
    "\n",
    "Save your output as `DHS_daytime.csv` and check that it is roughly the same as the file we have provided. There may be slight differences if you chose to calculate a different set of features than those described in 5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Fit a model of wealth as a function of basic daytime features\n",
    "- **INPUT**: \n",
    " - `data/model/DHS_daytime.csv`, from Step 5.2\n",
    "- **OUTPUT**: \n",
    " - R^2 of model\n",
    " \n",
    "As in 3.2, use 10-fold cross-validation to fit a model of cluster-level DHS wealth (your response/dependent variable) as a function of the nightlights data (your predictor/independent variables). Since you have a reasonably large number of predictor variables, you should use a model that incorporates some form of regularization (e.g., ridge regression, lasso regression, or a tree-based method).  Report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds).\n",
    "\n",
    "How does this model's performance compare to that of the nightlights model? Can you think of any other ways (aside from deep learning) that you might use to improve the features being extracted from daytime imagery, which might improve the predictive performance of your model?\n",
    "\n",
    "*Note:* Even if you do not choose to do parts 6-7, make sure you do parts 8 and 9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Replace this section with your observations and interpretation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. *Optional*: Use deep learning to extract fancier features\n",
    "\n",
    "This is where things get interesting. You will use existing libraries to extract more meaningful features from the daytime imagery, similar to what is shown in Fig. 2 of the paper.\n",
    "\n",
    "## 6.1. Use the keras library to use a basic CNN to extract features of the daytime images \n",
    " \n",
    "- **INPUT**: \n",
    " - `google_image/...`: Raw images, from Step 4\n",
    "- **OUTPUT**: \n",
    " - `google_image_features_cnn.csv`: Image features \n",
    "\n",
    "Begin by using a Convolutional Neural Network that has been pre-trained on ImageNet to extract features from the images. We recommend using the [`Keras` library](https://keras.io/), which provides a very straightforward interface to [TensorFlow](https://www.tensorflow.org/).\n",
    "\n",
    "Hints:\n",
    " - This [short intro](https://github.com/fchollet/deep-learning-models/blob/master/README.md) will help you get started with extracting features from the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2. Test whether these new features of satellite imagery can predict wealth\n",
    "- **INPUT**: \n",
    " - `google_image_features_cnn.csv`: Satellite imagery features, from Step 6.1\n",
    " - `rwanda_cluster_avg_asset_2010.csv`: DHS cluster averages, from Step 2\n",
    "- **OUTPUT**: \n",
    " - `data/model/DHS_daytime.csv`: Merged dataset with 492 rows, and 4097 columns (one indicates average cluster wealth, 4096 CNN-based features)\n",
    " - R^2 of model\n",
    " \n",
    "Calculate the average value of each feature for each of the DHS clusters. As in Step 3.1 and 5.2, you will want to aggregate over images near the cluster centroid by taking the average value for each feature. Create a scatterplot showing the relationship between average cluster wealth (y-axis) and the first principal component of all of your image features (x-axis) - in other words, run PCA on your 4096 image features and plot the first PC on the x-axis. Your scatterplot should have one dot for each of the 492 DHS clusters.\n",
    "\n",
    "Use 10-fold cross-validation to fit a model of cluster-level DHS wealth (your response/dependent variable) as a function of the \"deep\" features (your predictor/independent variables). Use a model that incorporates some form of regularization (e.g., ridge regression, lasso regression, or a tree-based method).  Report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. *Optional:* Replicate the transfer learning results of Jean et al (2016)\n",
    "\n",
    "The only thing missing at this point is the \"transfer learning\" step. In other words, instead of using the image features extracted by the CNN directly, we want to retrain the CNN to predict nightlights from daytime imagery, and use those features, which presumably are more appropriate to our final prediction task.\n",
    "\n",
    "## 7.1. Use the nightlights to retrain the CNN and extract features\n",
    "\n",
    "- **INPUT**: \n",
    " - `google_image/...`: Raw images, from Step 4\n",
    "- **OUTPUT**: \n",
    " - `google_image_features_cnn_retrained.csv`: Image features \n",
    "\n",
    "Following the approach used in the paper, first divide your daytime images into three groups, corresponding to images where the corresponding night-lights pixel is dim, medium, or bright. Use these values to define your groups: [0, 3), [3, 35), [35, 64). We have given you the code to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Test whether \"deep\" features of satellite imagery can predict wealth\n",
    "- **INPUT**: \n",
    " - `google_image_cnn/...`: Satellite images from 7.1\n",
    "- **OUTPUT**: \n",
    " - `data/model/DHS_CNN.csv`: Merged dataset with 492 rows, and 4097 columns (one indicates average cluster wealth, 4096 CNN features)\n",
    " - R^2 of model\n",
    "\n",
    "Repeat 6.2, except this time use the features generated from 7.1, i.e., the features that have been constructed after transfer learning. As in 6.2, show a scatterplot of the relationship between average cluster wealth (y-axis) and the first principal component of your image features. Then, report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Construct a high-resolution map of the  predicted wealth of Rwanda\n",
    "- **INPUT**: \n",
    " - Model, image features (data/model/features_all_predictimage_location.csv)\n",
    "- **OUTPUT**: \n",
    " - Map ('poverty_mapping.tiff')\n",
    " \n",
    "Choose your favorite model from the three daytime-based models that you have trained above: 5.3 (basic daytime features), 6.2 (deep daytime features), or 7.2 (transfer-learned daytime features). Use this model to calculate the predicted wealth of every one of your original images. Create a heatmap showing the distribution of predicted wealth in Rwanda. With any luck, it will look something like this:\n",
    "<img src=\"figure/pmap.png\" alt=\"Map\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step_9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Step back and interpret your results\n",
    "\n",
    "Now that you are more intimately familiar with what is being done in the Jean et al. paper, provide some intelligent/critical commentary on what you think of the overall method. If you were going to set about predicting wealth from satellite imagery, is there anything you would do differently, or do you think the recipe established in the paper is as good as can be? Would you trust the maps you've generated, if you needed to use them for a research project or to make policy recommendations? What do you think the limits of this approach are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Replace this section with your observations and interpretation*\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
